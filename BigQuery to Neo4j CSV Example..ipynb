{
  "metadata": {
    "name": "BigQuery to Neo4j CSV Example",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n\nimport pyspark\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\n\nfrom pyspark.sql import SparkSession\nspark \u003d SparkSession.builder\\\n  .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\")\\\n  .getOrCreate()\n\n# Documentation on how to read BigQuery tables from Spark: \n# https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example#pyspark\nchicago_crime \u003d spark.read.format(\u0027bigquery\u0027) \\\n  .option(\u0027table\u0027, \u0027bigquery-public-data:chicago_crime.crime\u0027) \\\n  .load()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%python\n\nimport pyspark\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.session import SparkSession\nfrom pyspark.sql.functions import col, monotonically_increasing_id\nfrom pyspark.sql.functions import sha2, concat_ws\n\n# Add unique location ID\nchicago_crime \u003d chicago_crime.withColumn(\"location_id\", sha2(concat_ws(\",\", col(\u0027block\u0027), col(\u0027latitude\u0027), col(\u0027longitude\u0027)), 256))\nchicago_crime.cache()\n\n# Decompose the BigQuery table into a series of \"node\" and \"relationship\" tables.\n# This follows the \"normalized loading\" approach, also called \"tables for labels\" which you can read\n# more about here: https://neo4j.com/developer/spark/architecture/#_normalized_loading\n#\n# Essentially: one table per node label, one table per rel type.\nnode_cases \u003d chicago_crime.select(\u0027unique_key\u0027, \u0027case_number\u0027, \u0027date\u0027, \u0027description\u0027, \u0027year\u0027)\nnode_location \u003d chicago_crime.select(\u0027location_id\u0027, \u0027block\u0027, \u0027latitude\u0027, \u0027longitude\u0027, \u0027beat\u0027, \u0027district\u0027, \u0027ward\u0027)\nnode_casetype \u003d chicago_crime.select(\u0027primary_type\u0027).distinct()\n\nrel_case_AT_location \u003d chicago_crime.select(\u0027unique_key\u0027, \u0027location_id\u0027)\nrel_case_TYPE_casetype \u003d chicago_crime.select(\u0027unique_key\u0027, \u0027primary_type\u0027)\n\nnode_cases.cache()\nnode_location.cache()\nnode_casetype.cache()\nrel_case_AT_location.cache()\nrel_case_TYPE_casetype.cache()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "# This paragraph fetches code that implements the Neo4jImportCSV helper we need in the next step\n# This is effectively the same as importing a library\nfrom urllib.request import urlopen\nlink \u003d \"https://raw.githubusercontent.com/moxious/gds-import/main/lib.py\"\nbuffer \u003d urlopen(link).read()\npython_code \u003d buffer.decode(\"utf-8\")\nexec(python_code)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\nhelper \u003d Neo4jImportCSV(spark, \"gs://meetup-data/\", \"chicago_crime_bigquery\")\n\nhelper.write_node_batch(df\u003dnode_cases, label\u003d\"Case\", key\u003d\u0027unique_key\u0027)\nhelper.write_node_batch(df\u003dnode_location, label\u003d\u0027Location\u0027, key\u003d\u0027location_id\u0027)\nhelper.write_node_batch(df\u003dnode_casetype, label\u003d\u0027CaseType\u0027, key\u003d\u0027primary_type\u0027)\n\nhelper.write_rel_batch(df\u003drel_case_AT_location, source_label\u003d\u0027Case\u0027, source_key\u003d\u0027unique_key\u0027, rel_type\u003d\u0027AT\u0027, target_label\u003d\u0027Location\u0027, target_key\u003d\u0027location_id\u0027)\nhelper.write_rel_batch(df\u003drel_case_TYPE_casetype, source_label\u003d\u0027Case\u0027, source_key\u003d\u0027unique_key\u0027, rel_type\u003d\u0027TYPE\u0027, target_label\u003d\u0027CaseType\u0027, target_key\u003d\u0027primary_type\u0027)\n\nprint(\"All done!\")"
    }
  ]
}